{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from darkflow.net.build import TFNet\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ./cfg/tiny-yolo-voc.cfg\n",
      "Parsing cfg/tiny-yolo-voc.cfg\n",
      "Loading bin/tiny-yolo-voc.weights ...\n",
      "Successfully identified 63471556 bytes\n",
      "Finished in 0.011000394821166992s\n",
      "Model has a VOC model name, loading VOC labels.\n",
      "\n",
      "Building net ...\n",
      "Source | Train? | Layer description                | Output size\n",
      "-------+--------+----------------------------------+---------------\n",
      "       |        | input                            | (?, 416, 416, 3)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 416, 416, 16)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 208, 208, 16)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 208, 208, 32)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 104, 104, 32)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 104, 104, 64)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 52, 52, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 52, 52, 128)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 26, 26, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 26, 26, 256)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 13, 13, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 512)\n",
      " Load  |  Yep!  | maxp 2x2p0_1                     | (?, 13, 13, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1    linear           | (?, 13, 13, 125)\n",
      "-------+--------+----------------------------------+---------------\n",
      "GPU mode with 1.0 usage\n",
      "Finished in 4.031230688095093s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "option = {\n",
    "    'model': 'cfg/tiny-yolo-voc.cfg',\n",
    "    'load': 'bin/tiny-yolo-voc.weights',\n",
    "    'threshold': 0.4,\n",
    "    'gpu': 1.0\n",
    "}\n",
    "\n",
    "# After choosing the options of the model we have to pass it to the CNN\n",
    "\n",
    "tfnet = TFNet(option) # Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('output.mp4'):\n",
    "    os.remove('output.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "#width=cap.set(cv2.CAP_PROP_FRAME_WIDTH, 416) \n",
    "#height=cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 416)\n",
    "\n",
    "\n",
    "width =int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height =int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    " \n",
    "#Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Be sure to use the lower case\n",
    "out = cv2.VideoWriter('output.mp4', fourcc, 20.0, (width, height))\n",
    "\n",
    "# Initialize all the arrays\n",
    "bbox_on_use=[]\n",
    "ok=[]\n",
    "tracker=[]\n",
    "areas=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS 1.7\n",
      "FPS 1.1\n",
      "FPS 0.7\n",
      "FPS 0.9\n",
      "FPS 1.6\n",
      "FPS 1.0\n",
      "FPS 1.1\n",
      "FPS 1.7\n",
      "FPS 1.3\n",
      "FPS 1.2\n",
      "FPS 1.2\n",
      "FPS 1.2\n",
      "FPS 1.3\n",
      "FPS 1.2\n",
      "FPS 1.2\n",
      "FPS 1.2\n",
      "FPS 1.2\n",
      "FPS 1.2\n",
      "FPS 1.1\n",
      "FPS 1.2\n",
      "FPS 1.2\n",
      "FPS 1.2\n",
      "FPS 1.4\n",
      "FPS 1.6\n",
      "FPS 1.7\n",
      "FPS 1.7\n",
      "FPS 1.7\n",
      "FPS 1.7\n",
      "FPS 1.7\n",
      "FPS 1.4\n",
      "FPS 1.2\n",
      "FPS 1.2\n",
      "FPS 1.3\n",
      "FPS 1.3\n",
      "FPS 1.3\n",
      "FPS 1.3\n",
      "FPS 1.2\n",
      "FPS 1.2\n",
      "FPS 1.2\n",
      "FPS 1.3\n",
      "FPS 1.3\n",
      "FPS 1.3\n",
      "FPS 1.2\n",
      "FPS 1.3\n",
      "FPS 1.3\n",
      "FPS 1.3\n",
      "FPS 1.3\n",
      "FPS 1.5\n",
      "FPS 1.7\n",
      "FPS 1.7\n",
      "FPS 1.7\n",
      "FPS 1.7\n",
      "FPS 1.7\n",
      "FPS 1.6\n",
      "FPS 1.7\n",
      "FPS 1.3\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    stime = time.time()\n",
    "    ret, frame = cap.read() # ret is true if the video is working or false if not\n",
    "    results = tfnet.return_predict(frame) \n",
    "    \n",
    "    #   If there are two people on the photo, reults give to us the following information:\n",
    "    #   [{'bottomright': {'x': 593, 'y': 320},\n",
    "    #  'confidence': 0.42716929,\n",
    "    #  'label': 'person',\n",
    "    #  'topleft': {'x': 417, 'y': 124}},\n",
    "    #  {'bottomright': {'x': 283, 'y': 373},\n",
    "    #  'confidence': 0.76005679,\n",
    "    #  'label': 'person',\n",
    "    #  'topleft': {'x': 177, 'y': 104}}]\n",
    "    \n",
    "    if ret:\n",
    "        bbox=[]        \n",
    "        for result in results:\n",
    "            tl = (result['topleft']['x'], result['topleft']['y']) # Top left corner coordinates\n",
    "            br = (result['bottomright']['x'], result['bottomright']['y']) # Bottom right corner coordinates\n",
    "            box=(tl[0],tl[1],br[0],br[1])\n",
    "            bbox.append(box)\n",
    "                \n",
    "        for i in range(len(bbox)):\n",
    "            new_bbox=True\n",
    "            \n",
    "            for x in range(len(bbox_on_use)):\n",
    "                th=100 # Threshold\n",
    "                corner=(bbox[i][0],bbox[i][1])\n",
    "                max_corner=(bbox_on_use[x][0]+th,bbox_on_use[x][1]+th)\n",
    "                min_corner=(bbox_on_use[x][0]-th,bbox_on_use[x][1]-th)\n",
    "                if corner<max_corner and corner>min_corner:\n",
    "                    new_bbox=False\n",
    "                    \n",
    "            if new_bbox==True:\n",
    "                tracker.append(cv2.TrackerKCF_create())\n",
    "                confir=tracker[-1].init(frame, bbox[i])\n",
    "                ok.append(confir)\n",
    "                bbox_on_use.append(bbox[i])\n",
    "                area=(bbox[i][2]-bbox[i][0])*(bbox[i][3]-bbox[i][1])\n",
    "                areas.append(area)\n",
    "                \n",
    "        # Delete all the tracker that are dead    \n",
    "        aux=0\n",
    "        y=False in ok\n",
    "        while y==True:\n",
    "            if ok[aux]==False:\n",
    "                del ok[aux]\n",
    "                del bbox_on_use[aux]\n",
    "                del tracker[aux]\n",
    "                del areas[aux]\n",
    "            else:\n",
    "                aux=aux+1\n",
    "                \n",
    "            y=False in ok \n",
    "            \n",
    "        # Update the tracker\n",
    "        for a in range(len(ok)):\n",
    "            ok[a], bbox_on_use[a]= tracker[a].update(frame)\n",
    "            ptl = (int(bbox_on_use[a][0]), int(bbox_on_use[a][1]))\n",
    "            pbr = (int(bbox_on_use[a][0] + bbox_on_use[a][2]), int(bbox_on_use[a][1] + bbox_on_use[a][3]))\n",
    "            frame=cv2.rectangle(frame, ptl, pbr, (255,0,0), 2, 1)\n",
    "            #cc=[int((pbr[0]-ptl[0])/2+ptl[0]),int((pbr[1]-ptl[1])/2+ptl[1])] # centre of the bounding box coodintes \n",
    "            #frame = cv2.circle(frame, (cc[0],cc[1]) , 2, (0,0,255), -1)\n",
    "            \n",
    "            # The distance was calculated following the steps showed in the next web:\n",
    "            # https://zone.biblio.laurentian.ca/bitstream/10219/2458/1/Peyman%20Alizadeh%20MSc.%20Thesis%20Corrected_2_2.pdf\n",
    "            area_b=(bbox_on_use[a][2]-bbox_on_use[a][0])*(bbox_on_use[a][3]-bbox_on_use[a][1])\n",
    "            movement_distance=0.01\n",
    "            if area_b>areas[a]:\n",
    "                d=movement_distance/(1-areas[a]/area_b)\n",
    "                text='%.2f' % d\n",
    "            elif area_b<areas[a]:\n",
    "                d=movement_distance/(1-area_b/areas[a])\n",
    "                text='%.2f' % d\n",
    "            else:\n",
    "                # First frame\n",
    "                text='loading'\n",
    "                \n",
    "            areas[a]=area_b\n",
    "            frame = cv2.putText(frame, text, ptl, cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 0), 1)\n",
    "                 \n",
    "        out.write(frame)\n",
    "        cv2.imshow('frame', frame)\n",
    "        print('FPS {:.1f}'.format(1 / (time.time() - stime)))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
